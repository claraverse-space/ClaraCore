# Use llama.cpp's official CUDA server image - has CUDA support built-in
FROM ghcr.io/ggml-org/llama.cpp:server-cuda

# Set working directory
WORKDIR /app

# Copy only the ClaraCore binary
COPY claracore /app/claracore

# Make it executable
RUN chmod +x /app/claracore

# Expose port
EXPOSE 5890

# Set the entrypoint
ENTRYPOINT ["/app/claracore"]
CMD ["-listen", "0.0.0.0:5890"]
